---
permalink: /
title: "Justin Kang"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Welcome to my website. You can find my <a href='publications'>Publications</a>, <a href='portfolio'>Projects</a> and <a href='files/Resume.pdf'>CV</a> above. Below, you can find some information about me.

* I am a Graduate Student at the University of California, Berkeley, affiliated with the BAIR Lab and supervised by <a href='https://people.eecs.berkeley.edu/~kannanr'>Prof. Kannan Ramchandran</a>.
* Previously, I completed my Master's of Applied Science at The University of Toronto supervised by <a href='https://www.comm.utoronto.ca/~weiyu/'>Prof. Wei Yu</a>.
* My current research focus is in *attribution* problems: trying to explain which components (input features, training data, etc.) are most significant in the prediction of a model (Such as an LLM). I think understanding these problems is critical to building better models that can faithfully explain themselves.
* I have a strong background in signal processing and information theory, and like to view machine learning problems through that lens, which often leads to unique and exciting solutions.
* I received my B.A.Sc. from the <a href='https://www.ubc.ca/'>University of British Columbia</a> in <a href="https://www.engphys.ubc.ca/">Engineering Physics</a>.

Recent News
======
* I'll be joining Suraj Srinivas, Jorge Piazentin Ono and Jared Evans this summer working on Autolabeling and data filtering problems at Bosch AI Research.
* Excited to present our recent research in interpreting LLMs at the <a href="https://arl.devcom.army.mil/">DEVCOM Army Research Lab</a>.
* Our paper *SPEX: Scaling Feature Interaction Explanations for LLMs* has been accepted to ICML! SPEX makes it easier to attribute importance in LLMs and other models, helping you interpret your models.
* Our paper *Learning to Understand: Identifying Interactions via the Mobius Transform* has been accepted to NeurIPS!
* A recent <a href="https://builtin.com/articles/what-is-federated-learning">article about Federated Learning</a> that I was interviewed for.
* I'm happy to anounce that my paper has won the <a href="https://www.itsoc.org/news/recipients-2024-ieee-communication-society-and-information-theory-society-joint-paper-award">2024 IEEE Communication Society and Information Theory Society Joint Paper Award</a>.
* I'll be joining Google for summer 2024 as a Student Reseacher, working on a project with the Google Cloud Platforms Systems Research Group (SRG) hosted by Kun Lin and Prof. David Culler.


Selected Papers
======

1. ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs. Butler, L(e), Agarwal, A.(e), Kang, J.(e), Erginbas Y.E.,  Ramchandran, K., Yu, B. Preprint. (<a href="https://arxiv.org/abs/2505.17495">paper</a>)*

2. SHAP-Zero explains biological sequence models with near-zero marginal cost for future queries. Tsui, D, Musharaf, A, Erginbas, Y.E.,  Kang, J.S., Aghazadeh. Preprint (<a href="https://arxiv.org/abs/2410.19236">paper</a>)

3. SPEX: Scaling Feature Interaction Explanations for LLMs. *Kang, J.S(e)., Butler, L.(e), Agarwal, A.(e),Erginbas Y.E., Pedarsani, R., Ramchandran, K., Yu, B.*, ICML, 2025. (<a href="https://arxiv.org/abs/2502.13870">paper</a>)

4. Learning to Understand: Identifying Interactions via the Mobius Transform. *Kang, J.S., Erginbas, Y.E., Butler, L., Pedarsani, R., Ramchandran, K. (2024)*, NeurIPS, 2024. (<a href="https://arxiv.org/abs/2402.02631">paper</a>, <a href="https://www.youtube.com/watch?v=5-OHk25H1mE">video</a>) 

5. Learning a 1-Layer Conditional Generative Model in Total Variation. *Ajil Jalal, Justin Singh Kang, Ananya Uppal, Kannan Ramchandran, Eric Price*. NeurIPS 2023. (<a href='https://openreview.net/forum?id=wImYhdu4VF'>paper</a>, <a href='https://nips.cc/virtual/2023/poster/70066'>video</a>, <a href='https://nips.cc/media/PosterPDFs/NeurIPS%202023/70066.png?t=1702321855.5320883'>poster</a>, <a href='https://github.com/basics-lab/learningGenerativeModels'>code</a>)

6. The Fair Value of Data Under Heterogeneous Privacy Constraints in Federated Learning. *Justin Singh Kang, Ramtin Pedarsani and Kannan Ramchandran*, NeurIPS FL@FM 2023, TMLR, 2024. (<a href='https://arxiv.org/abs/2301.13336'>paper</a>, <a href='https://www.youtube.com/watch?v=S_DBTIlaodE'>video</a>)

7. Efficiently Computing Sparse Fourier Transforms of q-ary Functions. *Justin Singh Kang(e), Y. E. Erginbas(e), A. Aghazadeh and K. Ramchandran*. IEEE ISIT 2023 (<a href='https://ieeexplore.ieee.org/document/10206686'>paper</a>, <a href='https://www.youtube.com/watch?v=_UgRE1iSrzY&t=2s'>video</a>, <a href='https://github.com/basics-lab/qsft'>code</a>)
